{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266874880\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import blimpy as bl\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy import stats\n",
    "from scipy.interpolate import interp1d\n",
    "from astropy.stats import sigma_clip\n",
    "import setigen as stg\n",
    "from astropy import units as u\n",
    "import h5py\n",
    "import psutil\n",
    "from numba import jit\n",
    "import multiprocessing\n",
    "import logging\n",
    "%matplotlib inline\n",
    "\n",
    "process = psutil.Process()\n",
    "\n",
    "print(process.memory_info().rss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carmen's data\n",
    "\n",
    "csv_file = '/home/cgchoza/galaxies/complete_cadences_catalog.csv'\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Personal notes:\n",
    "# The distribution of these cadences isn't necessarily intuitive;\n",
    "# there are 459 of them total, but more than 459*6 files\n",
    "# because some are spliced and some not,\n",
    "# so some cadences have 6*7 or 6*8 files, 6 for each node,\n",
    "# etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab only L-band spectra\n",
    "\n",
    "dfl = df.iloc[np.where(df['Band'].values == 'L')[0]]\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jit\n",
    "def make_snippets(input_arr, freqs_list, half_window):\n",
    "    snippet_array = [input_arr[:,:,ctrfreq-half_window:ctrfreq+half_window] for ctrfreq in freqs_list]\n",
    "    return snippet_array\n",
    "\n",
    "# helper function from B. Brzycki to calculate signal bandwidths\n",
    "\n",
    "def threshold_baseline_bounds(spec, p=0.01):\n",
    "    \"\"\"\n",
    "    Create bounds based on integrated intensity on either side of the central\n",
    "    peak, as a fraction of the peak integrated intensity. Uses a 1D fit to the\n",
    "    noise baseline.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    spec : ndarray\n",
    "        Intensity spectra\n",
    "    p : float, optional\n",
    "        Fraction of peak, used to set left and right bounds\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    l : int\n",
    "        Left bound\n",
    "    r : int\n",
    "        Right bound\n",
    "    metadata : dict\n",
    "        Dictionary with metadata. Contains noise mean and spectra maximum,\n",
    "        which are used to normalize spec to the spectra maximum.\n",
    "    \"\"\"\n",
    "    noise_spec = sigma_clip(spec, masked=True)\n",
    "    x = np.arange(len(spec))\n",
    "\n",
    "    coeffs = np.polyfit(x[~noise_spec.mask], noise_spec[~noise_spec.mask], 1)\n",
    "    poly = np.poly1d(coeffs)\n",
    "    \n",
    "    spec = spec - poly(x)\n",
    "    norm_spec = (spec ) / (np.max(spec) )\n",
    "    \n",
    "    cutoffs = np.where(norm_spec < p)[0]\n",
    "    \n",
    "    peak = np.argmax(norm_spec)\n",
    "    i = np.digitize(peak, cutoffs) - 1\n",
    "    # print(i, i+1, len(cutoffs), len(spec), cutoffs[-1])\n",
    "    l = cutoffs[i] + 1\n",
    "    if i+1 < len(cutoffs):  # my edit from Bryan's function, intended as a failsafe when bandwidths are too large\n",
    "        r = cutoffs[i + 1]\n",
    "    else:\n",
    "        r = cutoffs[i]\n",
    "    \n",
    "    metadata = {\n",
    "        'noise_mean': np.mean(noise_spec),\n",
    "        'spec_max': np.max(spec)\n",
    "    }\n",
    "    return l, r, metadata\n",
    "\n",
    "### parameter pulling function\n",
    "\n",
    "def get_parameters(inp_dat):\n",
    "\n",
    "    input_arr = inp_dat[0]\n",
    "    ctrfreq = inp_dat[1]\n",
    "    drift = inp_dat[2]\n",
    "\n",
    "    f_len = input_arr.shape[1]\n",
    "    power_spectrum = np.average(input_arr[:, f_len//2-caleb_wl_in_idxs//2:f_len//2+caleb_wl_in_idxs//2], axis=0)\n",
    "    power_spectrum = (power_spectrum-np.median(power_spectrum)) # subtract\n",
    "    power_spectrum = power_spectrum / power_spectrum.max()      # and divide\n",
    "    sk = skew(power_spectrum)\n",
    "    ku = kurtosis(power_spectrum, fisher=False)\n",
    "    sarle = (sk**2 + 1) / ku\n",
    "    fstd = np.std(power_spectrum)\n",
    "    \n",
    "    time_series = np.average(input_arr, axis=1)\n",
    "    normalized_time_series = (time_series-np.median(time_series))/np.max(time_series-np.median(time_series))\n",
    "    tsk = skew(normalized_time_series)\n",
    "    tstd = np.std(normalized_time_series)\n",
    "\n",
    "    # calculate spectral kurtosis for varying spectral window size\n",
    "\n",
    "    window_lengths = np.logspace(np.log10(0.0002), np.log10(0.1000), 50) # from 2 kHz to 100 kHz\n",
    "    mini_kurts = []\n",
    "\n",
    "    full_power_spectrum = np.average(input_arr, axis=0)\n",
    "\n",
    "    for wl in window_lengths:\n",
    "        wl_in_idxs = round(wl/np.abs(foff))\n",
    "        #print(wl_in_idxs//2)\n",
    "        ps_slice = full_power_spectrum[f_len//2-wl_in_idxs//2:f_len//2+wl_in_idxs//2]\n",
    "        ps_slice = ps_slice / ps_slice.max()\n",
    "        mk = kurtosis(ps_slice, fisher=False)\n",
    "        mini_kurts.append(mk)\n",
    "\n",
    "    if (np.argmax(mini_kurts) == len(mini_kurts) - 1) or (np.argmax(mini_kurts) == 0):\n",
    "        tbw = window_lengths[np.argmax(mini_kurts)]\n",
    "    else:\n",
    "        idx_max = np.argmax(mini_kurts)\n",
    "        xxx = np.log10(1e6*np.array(window_lengths))\n",
    "        x_m = xxx[idx_max-1:idx_max+2]\n",
    "        y_m = mini_kurts[idx_max-1:idx_max+2]\n",
    "\n",
    "        # linear algebra to find best-fit parabola\n",
    "        denom = (x_m[0] - x_m[1])*(x_m[0] - x_m[2])*(x_m[1] - x_m[2])\n",
    "        a = (x_m[2] * (y_m[1] - y_m[0]) + x_m[1] * (y_m[0] - y_m[2]) + x_m[0] * (y_m[2] - y_m[1])) / denom\n",
    "        b = (x_m[2]**2 * (y_m[0] - y_m[1]) + x_m[1]**2 * (y_m[2] - y_m[0]) + x_m[0]**2 * (y_m[1] - y_m[2])) / denom\n",
    "        c = (x_m[1] * x_m[2] * (x_m[1] - x_m[2]) * y_m[0] + x_m[2] * x_m[0] * (x_m[2] - x_m[0]) * y_m[1] + x_m[0] * x_m[1] * (x_m[0] - x_m[1]) * y_m[2]) / denom\n",
    "\n",
    "        # vertex coords\n",
    "        x0 = -b / (2 * a)\n",
    "        #y0 = a * x0**2 + b * x0 + c\n",
    "\n",
    "        tbw = 10**x0 / 1e6\n",
    "        \n",
    "    subset_kurtoses = np.array(mini_kurts)[np.where(window_lengths <= caleb_wl)[0]]\n",
    "    subset_windows = window_lengths[np.where(window_lengths <= caleb_wl)[0]]\n",
    "\n",
    "    corr = stats.pearsonr(np.log10(subset_windows), subset_kurtoses)[0]\n",
    "\n",
    "    sigbw_calc_ds = input_arr[:,f_len//2-sigbw_wl_in_idxs//2:f_len//2+sigbw_wl_in_idxs//2]\n",
    "        \n",
    "    frame = stg.Frame.from_data(df=np.abs(foff)*u.MHz,\n",
    "                            dt=18.253611008*u.s,\n",
    "                            fch1=(ctrfreq-sigbw_calc_wl/2)*u.MHz,\n",
    "                            ascending=True, # fch1 is a minimum\n",
    "                            data=sigbw_calc_ds)\n",
    "    \n",
    "    dd_fr = stg.dedrift(frame, -drift) # not sure why the negative is needed, but it works for me\n",
    "\n",
    "    spec = stg.integrate(dd_fr)\n",
    "\n",
    "    l, r, _ = threshold_baseline_bounds(spec)\n",
    "\n",
    "    sigbw = (float(r)-float(l))*np.abs(foff)\n",
    "\n",
    "    #0: frequency [MHz]\n",
    "    #1: drift rate [Hz/s]\n",
    "    #2: SNR\n",
    "    #3: spectral skewness\n",
    "    #4: spectral kurtosis\n",
    "    #5: Sarle's coefficient\n",
    "    #6: correlation coefficient [kurtosis vs. log(bandwidth)]\n",
    "    #7: turning-point bandwidth [Hz]\n",
    "    #8: temporal skewness\n",
    "    #9: time-series standard deviation\n",
    "    #10: power-spectrum standard deviation\n",
    "    #11: signal bandwidth\n",
    "\n",
    "    if len(np.array([sk, ku, sarle, corr, tbw, tsk, tstd, fstd, sigbw])) != 9:\n",
    "        print(f'Error! Not all parameters calculated. Check hit at freq = {ctrfreq}.')\n",
    "\n",
    "    return np.array([sk, ku, sarle, corr, tbw, tsk, tstd, fstd, sigbw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14608822272\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(process.memory_info().rss)\n",
    "\n",
    "caleb_wl = 0.002700 # 2.7 kHz\n",
    "sigbw_calc_wl = 0.005000 # 5 kHz\n",
    "full_wl = 0.100000 # 100 kHz\n",
    "\n",
    "foff = -2.7939677238464355e-06\n",
    "\n",
    "caleb_wl_in_idxs = round(caleb_wl/np.abs(foff))\n",
    "sigbw_wl_in_idxs = round(sigbw_calc_wl/np.abs(foff))\n",
    "full_wl_in_idxs = round(full_wl/np.abs(foff))\n",
    "\n",
    "def process_files(xxx):\n",
    "\n",
    "    code = xxx[0]\n",
    "    df0 = xxx[1]\n",
    "\n",
    "    # Remove all handlers associated with the root logger object.\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "\n",
    "    logging.basicConfig(filename=f'/datax/scratch/benjb/{code}.log', filemode=\"a\", level=logging.DEBUG)\n",
    "\n",
    "    param_array = []\n",
    "\n",
    "    for iii in range(len(df0)):\n",
    "\n",
    "        logger.info(f'File {iii+1} out of {len(df0)}')\n",
    "\n",
    "        bytes_available = psutil.virtual_memory()[1]\n",
    "        if bytes_available <= 32e9:\n",
    "            logger.info(f'Memory dangerously low: {bytes_available} bytes remaining. Breaking ...')\n",
    "            break\n",
    "\n",
    "        f = h5py.File(df0['.h5 path'].values[iii], 'r')\n",
    "        dset = f['data']\n",
    "\n",
    "        fch1 = dset.attrs['fch1']\n",
    "        foff = dset.attrs['foff']\n",
    "\n",
    "        freqs = np.linspace(fch1, fch1+foff*dset.shape[2], dset.shape[2])\n",
    "\n",
    "        dat_path = df0['.dat path'].values[iii]\n",
    "\n",
    "        df_dat = pd.read_table(dat_path, sep='\\s+', names=['Top_Hit_#','Drift_Rate','SNR','Uncorrected_Frequency','Corrected_Frequency',\n",
    "                                                    'Index', 'freq_start', 'freq_end', 'SEFD', 'SEFD_freq', 'Coarse_Channel_Number', \n",
    "                                                    'Full_number_of_hits'], skiprows=9)\n",
    "            \n",
    "        ctrfreqs = df_dat['Uncorrected_Frequency'].values\n",
    "        drifts = df_dat['Drift_Rate'].values\n",
    "\n",
    "        ctrfreqs_in_idxs0 = (ctrfreqs - fch1)/foff\n",
    "\n",
    "        ctrfreqs_in_idxs = ctrfreqs_in_idxs0[np.where((\n",
    "            ctrfreqs_in_idxs0 > full_wl_in_idxs//2) & (ctrfreqs_in_idxs0 < dset.shape[2]-full_wl_in_idxs//2))[0]]\n",
    "        \n",
    "        if len(ctrfreqs_in_idxs0) != len(ctrfreqs_in_idxs):\n",
    "            logger.info(f'{len(ctrfreqs_in_idxs0)-len(ctrfreqs_in_idxs)} hit(s) removed for being too close to edge.')\n",
    "            logger.info(f'Len ctrfreqs = {len(ctrfreqs)}.')\n",
    "            ctrfreqs = ctrfreqs[np.where((\n",
    "                ctrfreqs_in_idxs0 > full_wl_in_idxs//2) & (ctrfreqs_in_idxs0 < dset.shape[2]-full_wl_in_idxs//2))[0]]\n",
    "            drifts = drifts[np.where((\n",
    "                ctrfreqs_in_idxs0 > full_wl_in_idxs//2) & (ctrfreqs_in_idxs0 < dset.shape[2]-full_wl_in_idxs//2))[0]]\n",
    "            logger.info(f'Len ctrfreqs after chopping = {len(ctrfreqs)}.')\n",
    "        \n",
    "        snippets = make_snippets(dset, ctrfreqs_in_idxs.astype(int), full_wl_in_idxs//2)\n",
    "\n",
    "        #logger.info([snippet.shape for snippet in snippets])\n",
    "\n",
    "        snippets = np.squeeze(np.array(snippets), axis=2)\n",
    "\n",
    "        #logger.info([snippet.shape for snippet in snippets])\n",
    "\n",
    "        inputs = [[snippets[i], ctrfreqs[i], drifts[i]] for i in range(len(ctrfreqs_in_idxs))]\n",
    "        vecs = map(get_parameters, inputs)\n",
    "        params = np.array(list(vecs))\n",
    "\n",
    "        logger.info(params.shape)\n",
    "\n",
    "        param_array.append(params)\n",
    "\n",
    "        #logger.info(process.memory_info().rss)\n",
    "\n",
    "    np.save(f'/datax/scratch/benjb/{code}_params.npy', param_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/numpy/core/_asarray.py:171: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "#idx = [909*4+23, 909*4+24]\n",
    "#\n",
    "#dfl.iloc[idx]\n",
    "#\n",
    "#process_files(['test_batch', dfl.iloc[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define n sets of files\n",
    "    batch_size = len(dfl)//6\n",
    "    file_sets = [\n",
    "        ['batch_1', dfl.iloc[0:1*batch_size]],\n",
    "        ['batch_2', dfl.iloc[1*batch_size:2*batch_size]],\n",
    "        ['batch_3', dfl.iloc[2*batch_size:3*batch_size]],\n",
    "        ['batch_4', dfl.iloc[3*batch_size:4*batch_size]],\n",
    "        ['batch_5', dfl.iloc[4*batch_size:5*batch_size]],\n",
    "        ['batch_6', dfl.iloc[5*batch_size:]]\n",
    "    ]\n",
    "\n",
    "    # Create a pool of n processes\n",
    "    with multiprocessing.Pool(processes=6) as pool:\n",
    "        # Map the process_files function to each file set\n",
    "        pool.map(process_files, file_sets)\n",
    "\n",
    "    print(\"All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
